{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os, time, scipy.io\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "from optparse import OptionParser\n",
    "import numpy as np\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "from torch.autograd import Function, Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import rawpy\n",
    "sys.path.append(os.path.abspath(\"./dwt/*\"))\n",
    "from pytorch_wavelets.dwt import transform2d as dwt\n",
    "%matplotlib inline\n",
    "\n",
    "input_dir = './dataset/Sony/short/'\n",
    "gt_dir = './dataset/Sony/long/'\n",
    "checkpoint_dir = './result_Sony/'\n",
    "result_dir = './result_Sony/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    deviceTag = torch.device('cuda')\n",
    "else:\n",
    "    deviceTag = torch.device('cpu')\n",
    "print(deviceTag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train IDs\n",
    "train_fns = glob.glob(gt_dir + '0*.ARW')\n",
    "train_ids = [int(os.path.basename(train_fn)[0:5]) for train_fn in train_fns]\n",
    "\n",
    "ps = 512  # patch size for training\n",
    "save_freq = 500\n",
    "\n",
    "DEBUG = 0\n",
    "if DEBUG == 1:\n",
    "    save_freq = 2\n",
    "    train_ids = train_ids[0:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# DEFINE SINGLE_CONV CLASS\n",
    "class single_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) '''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(single_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ReplicationPad2d((1,1,1,1)),\n",
    "            nn.Conv2d(in_ch,out_ch,3), #(channel in, channel out, filter)\n",
    "            nn.BatchNorm2d(out_ch), # Channels in\n",
    "            nn.LeakyReLU(out_ch), #Channel in\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# DEFINE DOWN CLASS\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.conv1 = single_conv(in_ch,out_ch) # use previously defined single_cov\n",
    "        self.conv2 = single_conv(out_ch,out_ch) # use previously defined single_cov\n",
    "        self.down =  nn.MaxPool2d( (2,2))# use nn.MaxPool2d( )\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        xpre = self.conv2(x)\n",
    "        x = self.down(xpre)\n",
    "        return x, xpre\n",
    "    \n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# DEFINE UP CLASS\n",
    "# Note that this class will not only upsample x1, but also concatenate up-sampled x1 with x2 to generate the final output\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up, self).__init__()       \n",
    "        self.up =  nn.ConvTranspose2d(int(in_ch/2),int(in_ch/2),kernel_size=2, stride=2) #(channel in, channel out, filter)\n",
    "        self.conv = single_conv(in_ch,out_ch) # use previously defined single_cov\n",
    "        self.conv2 = single_conv(out_ch,out_ch) # use previously defined single_cov\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1); # Conv Transpose For upsampling       \n",
    "        #diffY = x2.size()[2] - x1.size()[2]\n",
    "        #diffX = x2.size()[3] - x1.size()[3]\n",
    "        #Idon't think I need any of this since we are already padding the begining nad end. \n",
    "#        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    " #                       diffY // 2, diffY - diffY//2))\n",
    "        \n",
    "        # Now we concatenat x2 channels with x1 channels\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# DEFINE OUTCONV CLASS\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ReplicationPad2d((1,1,1,1)),\n",
    "            nn.Conv2d(in_ch,out_ch,3), #(channel in, channel out, filter)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DWT Wavelet Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Wavelet Modules for discrete Wavelet Transform. We use a previous wavelet library to do the transformation\n",
    "##Wavelets are equivalent to Downsamples and upsamples in the LL domain, but are invertible operations and \n",
    "\n",
    "class dwtDown(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(dwtDown, self).__init__()\n",
    "        self.conv1 = single_conv(in_ch,out_ch) # use previously defined single_cov\n",
    "        self.conv2 = single_conv(out_ch,out_ch) # use previously defined single_cov\n",
    "        self.down = dwt.DWTForward()\n",
    "        self.conv3 = single_convPad(in_ch*4, out_ch) # use previously defined single_cov\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        xpre = self.conv2(x)\n",
    "        x = self.down(xpre)\n",
    "        xh, yl = self.down(x)\n",
    "        yl = yl[0]\n",
    "        x = torch.cat([xh, yl[:,:,0,:,:],yl[:,:,1,:,:],yl[:,:,2,:,:]], dim=1)\n",
    "        x = self.conv3(x)\n",
    "        return x, xpre\n",
    "\n",
    "    \n",
    "class dwtUp(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(dwtUp, self).__init__()\n",
    "        self.up = dwt.DWTInverse()\n",
    "        self.conv1 = single_convPad(int(in_ch/4), out_ch) # use previously defined single_cov\n",
    "        self.conv2 = single_convPad(out_ch, out_ch) # use previously defined single_cov\n",
    "    def forward(self, x,x2):\n",
    "        _,c,_,_ = x.shape\n",
    "        yl = []\n",
    "        bat = int(c/4)\n",
    "        xl = x[:,0:bat,:,:]\n",
    "        xh = x.unsqueeze(2)\n",
    "        xh = torch.cat([xh[:,bat:2*bat,:,:,:],xh[:,2*bat:3*bat,:,:,:],xh[:,3*bat:4*bat,:,:,:]], dim=2)\n",
    "        yl.append(xh)\n",
    "        x = self.up((xl,yl))\n",
    "        if x.shape != x2.shape:\n",
    "            x1 = transforms.functional.resize(x, x2.shape[2:])\n",
    "        x = torch.cat([x2, x], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNET structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ [TODO] ###################################################\n",
    "# Build your network with predefined classes: single_conv, up, down, outconv\n",
    "# The number of input and output channels should follow the U-Net Structure shown above.\n",
    "import torch.nn.functional as F\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = single_conv(n_channels, 32) # conv2d +  batchnorm + relu\n",
    "        self.inc2 = single_conv(32, 32) # conv2d +  batchnorm + relu\n",
    "        self.pool =  nn.MaxPool2d( (2,2))# use nn.MaxPool2d( )\n",
    "        self.down1 = dwtDown(32, 64)         # maxpool2d + conv2d + batchnorm + relu\n",
    "        self.down2 = dwtDown(64,128)                  # maxpool2d + conv2d + batchnorm + relu\n",
    "        self.down3 = dwtDown(128,256)                  # maxpool2d + conv2d + batchnorm + relu\n",
    "        self.down4 = dwtDown(256,512)                  # maxpool2d + conv2d + batchnorm + relu\n",
    "        self.up1 = dwtUp(1024,256)                    # upsample + pad + conv2d + batchnorm + relu\n",
    "        self.up2 = dwtUp(512,128)                    # upsample + pad + conv2d + batchnorm + relu\n",
    "        self.up3 = dwtUp(256,64)                    # upsample + pad + conv2d + batchnorm + relu\n",
    "        self.up4 = dwtUp(128,32)                    # upsample + pad + conv2d + batchnorm + relu\n",
    "        self.up5 = dwtUp(64,32)                    # upsample + pad + conv2d + batchnorm + relu\n",
    "        self.outc = outconv(32,12)                   # conv2d\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.inc(x)\n",
    "        x0 = self.inc2(x0)\n",
    "        x = self.pool(x0)\n",
    "        x,x1 = self.down1(x)\n",
    "        x,x2 = self.down2(x)\n",
    "        x,x3 = self.down3(x)\n",
    "        x,x4 = self.down4(x)\n",
    "        x = self.up1(x,x4)\n",
    "        x = self.up2(x,x3)\n",
    "        x = self.up3(x,x2)\n",
    "        x = self.up4(x,x1)\n",
    "        x = self.up5(x,x0)\n",
    "        x = self.outc(x)\n",
    "        x = F.pixel_shuffle(x,2) ## Paper final step rearranges 12 channes to 3 RGB channels\n",
    "        x = F.hardtanh(x, min_val=0, max_val=1) #Clamp the top and bottom to 0,1 since pixels can only be in this value\n",
    "        return x\n",
    "    \n",
    "    ## Helper function to load partial states\n",
    "    def load_my_state_dict(self, state_dict):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name not in own_state:\n",
    "                 continue\n",
    "            #if isinstance(param, self.Parameter):\n",
    "            else:\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions for packing raw and saving images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_raw(raw):\n",
    "    # pack Bayer image to 4 channels\n",
    "    im = raw.raw_image_visible.astype(np.float32)\n",
    "    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level\n",
    "\n",
    "    im = np.expand_dims(im, axis=2)\n",
    "    img_shape = im.shape\n",
    "    H = img_shape[0]\n",
    "    W = img_shape[1]\n",
    "\n",
    "    out = np.concatenate((im[0:H:2, 0:W:2, :],\n",
    "                          im[0:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 1:W:2, :],\n",
    "                          im[1:H:2, 0:W:2, :]), axis=2)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data takes long time to load. Keep them in memory after loaded.\n",
    "gt_images = [None] * 6000\n",
    "input_images = {}\n",
    "input_images['300'] = [None] * len(train_ids)\n",
    "input_images['250'] = [None] * len(train_ids)\n",
    "input_images['100'] = [None] * len(train_ids)\n",
    "\n",
    "g_loss = np.zeros((5000, 1))\n",
    "\n",
    "allfolders = glob.glob(result_dir + '*0')\n",
    "epochs = 4000\n",
    "for folder in allfolders:\n",
    "    lastepoch = np.maximum(epochs, int(folder[-4:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(4);\n",
    "#PATH = './results_Sony/sony4000.pth' #Using grad2\n",
    "#model.load_my_state_dict(torch.load(PATH,deviceTag))\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the number of parameters good for characterizing the size\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "model_save_path = './results_Sony/'  # directory to same the model after each epoch. \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "# note that although we want to use DICE for evaluation, we use BCELoss for training in this example\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.1) #Step Scheduler. \n",
    "TrainingLossData = np.zeros(epochs)\n",
    "# The loss function we use is binary cross entropy: nn.BCELoss()\n",
    "#criterion = nn.L1Loss()\n",
    "criterion = nn.MSELoss()\n",
    "# note that although we want to use DICE for evaluation, we use BCELoss for training in this example\n",
    "trainF= open(\"./epoch_loss/TrainingEpoch.txt\",\"w+\")\n",
    "\n",
    "################################################ [TODO] ###################################################\n",
    "# Start training\n",
    "for epoch in range(857,epochs):\n",
    "    print('Starting epoch {}/{}.'.format(epoch + 1, epochs))\n",
    "    epoch_loss = 0 ## Set Epoch Loss\n",
    "    #count = 0;\n",
    "    ##This version has a batch size of 1. In the future conside increasing batchsize\n",
    "    for ind in np.random.permutation(len(train_ids)):\n",
    "        # get the path from image id\n",
    "        train_id = train_ids[ind]\n",
    "        in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id)\n",
    "        in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n",
    "        in_fn = os.path.basename(in_path)\n",
    "\n",
    "        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id)\n",
    "        gt_path = gt_files[0]\n",
    "        gt_fn = os.path.basename(gt_path)\n",
    "        in_exposure = float(in_fn[9:-5])\n",
    "        gt_exposure = float(gt_fn[9:-5])\n",
    "        ratio = min(gt_exposure / in_exposure, 300)\n",
    "\n",
    "        st = time.time()\n",
    "\n",
    "        if input_images[str(ratio)[0:3]][ind] is None:\n",
    "            raw = rawpy.imread(in_path)\n",
    "            input_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis=0) * ratio\n",
    "\n",
    "            gt_raw = rawpy.imread(gt_path)\n",
    "            im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
    "            gt_images[ind] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n",
    "\n",
    "        # crop\n",
    "        H = input_images[str(ratio)[0:3]][ind].shape[1]\n",
    "        W = input_images[str(ratio)[0:3]][ind].shape[2]\n",
    "\n",
    "        xx = np.random.randint(0, W - ps)\n",
    "        yy = np.random.randint(0, H - ps)\n",
    "        input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps, xx:xx + ps, :]\n",
    "        gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n",
    "\n",
    "        if np.random.randint(2, size=1)[0] == 1:  # random flip\n",
    "            input_patch = np.flip(input_patch, axis=1)\n",
    "            gt_patch = np.flip(gt_patch, axis=1)\n",
    "        if np.random.randint(2, size=1)[0] == 1:\n",
    "            input_patch = np.flip(input_patch, axis=2)\n",
    "            gt_patch = np.flip(gt_patch, axis=2)\n",
    "        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n",
    "            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
    "            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n",
    "        #(1, 512, 512, 4)\n",
    "        #(1, 1024, 1024, 3)\n",
    "        input_patch = np.transpose(input_patch, (0,3,1,2))\n",
    "        input_patch = torch.from_numpy(input_patch.copy()).cuda()\n",
    "        gt_patch = np.transpose(gt_patch, (0,3,1,2))\n",
    "        gt_patch = torch.from_numpy(gt_patch.copy()).cuda()\n",
    "\n",
    "        img_pred = model.forward(input_patch)\n",
    "        loss = criterion(img_pred, gt_patch)\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ##print(epoch_loss/count);\n",
    "        \n",
    "    scheduler.step()\n",
    "    print('Epoch finished ! Loss: {}'.format(epoch_loss / len(train_ids)))\n",
    "    trainF.write('Epoch finished ! Loss: {}\\r\\n'.format(epoch_loss / len(train_ids)))\n",
    "    TrainingLossData[epoch] = epoch_loss / len(train_ids) ## Save for plotting\n",
    "    ################################################ [TODO] ###################################################\n",
    "    # Perform validation with eval_net() on the validation data\n",
    "    # Save the model after every 10 epochs. This save our Memory on HPC.\n",
    "    ##Save Top results after 95%\n",
    "    if epoch > epochs*0.95:\n",
    "        if os.path.isdir(model_save_path):\n",
    "            torch.save(model.state_dict(),model_save_path + 'sony{}.pth'.format(epoch + 1))\n",
    "        else:\n",
    "            os.makedirs(model_save_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(),model_save_path + 'sony{}.pth'.format(epoch + 1))\n",
    "        print('Checkpoint {} saved !'.format(epoch + 1))\n",
    "trainF.close() ## Close your files to write to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainF.close() ## Close your files to write to them\n",
    "torch.save(model.state_dict(),model_save_path + 'sony{}.pth'.format(857 + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to display 10 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading parameters Check network is correct\n",
    "PATH = './results_Sony/Imp_Models/Baseline_Sony_training.pth' #Using grad2\n",
    "model = UNet(4);\n",
    "model.load_state_dict(torch.load(PATH,map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0;\n",
    "model.eval();\n",
    "for ind in np.random.permutation(len(train_ids)):\n",
    "    with torch.no_grad():\n",
    "        # get the path from image id\n",
    "        train_id = train_ids[ind]\n",
    "        in_files = glob.glob(input_dir + '%05d_00*.ARW' % train_id)\n",
    "        in_path = in_files[np.random.random_integers(0, len(in_files) - 1)]\n",
    "        in_fn = os.path.basename(in_path)\n",
    "\n",
    "        gt_files = glob.glob(gt_dir + '%05d_00*.ARW' % train_id)\n",
    "        gt_path = gt_files[0]\n",
    "        gt_fn = os.path.basename(gt_path)\n",
    "        in_exposure = float(in_fn[9:-5])\n",
    "        gt_exposure = float(gt_fn[9:-5])\n",
    "        ratio = min(gt_exposure / in_exposure, 300)\n",
    "\n",
    "        st = time.time()\n",
    "\n",
    "        if input_images[str(ratio)[0:3]][ind] is None:\n",
    "            raw = rawpy.imread(in_path)\n",
    "            input_images[str(ratio)[0:3]][ind] = np.expand_dims(pack_raw(raw), axis=0) * ratio\n",
    "\n",
    "            gt_raw = rawpy.imread(gt_path)\n",
    "            im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n",
    "            gt_images[ind] = np.expand_dims(np.float32(im / 65535.0), axis=0)\n",
    "\n",
    "        # crop\n",
    "        H = input_images[str(ratio)[0:3]][ind].shape[1]\n",
    "        W = input_images[str(ratio)[0:3]][ind].shape[2]\n",
    "\n",
    "        xx = np.random.randint(0, W - ps)\n",
    "        yy = np.random.randint(0, H - ps)\n",
    "        input_patch = input_images[str(ratio)[0:3]][ind][:, yy:yy + ps, xx:xx + ps, :]\n",
    "        gt_patch = gt_images[ind][:, yy * 2:yy * 2 + ps * 2, xx * 2:xx * 2 + ps * 2, :]\n",
    "\n",
    "        if np.random.randint(2, size=1)[0] == 1:  # random flip\n",
    "            input_patch = np.flip(input_patch, axis=1)\n",
    "            gt_patch = np.flip(gt_patch, axis=1)\n",
    "        if np.random.randint(2, size=1)[0] == 1:\n",
    "            input_patch = np.flip(input_patch, axis=2)\n",
    "            gt_patch = np.flip(gt_patch, axis=2)\n",
    "        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n",
    "            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n",
    "            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n",
    "        #(1, 512, 512, 4)\n",
    "        #(1, 1024, 1024, 3)\n",
    "        input_patch = np.transpose(input_patch, (0,3,1,2))\n",
    "        input_patch = torch.from_numpy(input_patch.copy())#.cuda()\n",
    "        gt_patch = np.transpose(gt_patch, (0,3,1,2))\n",
    "        gt_patch = torch.from_numpy(gt_patch.copy())#.cuda()\n",
    "\n",
    "        img_pred = model.forward(input_patch)\n",
    "        count+=1;\n",
    "        gt_patch = gt_patch[0].cpu()\n",
    "        img_pred = img_pred[0].cpu()\n",
    "        gt_patch = np.transpose(gt_patch,(1,2,0))\n",
    "        img_pred = np.transpose(img_pred,(1,2,0))\n",
    "        plt.figure()\n",
    "        plt.imshow(gt_patch)\n",
    "        plt.title(\"Ground Truth\")\n",
    "        plt.figure()\n",
    "        plt.imshow(img_pred)\n",
    "        plt.title(\"Predicted patch\")\n",
    "    if count == 10: ## Change this number to change the number of patches shown. \n",
    "        break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
